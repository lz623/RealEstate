{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Zip Code"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "import mywebdriver as webdriver\n",
    "import mywebdriver\n",
    "import pandas as pd\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Price by zipcode"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "url = 'https://www.zip-codes.com/state/tx.asp'\n",
    "soup = webdriver.get_soup(url, driver_type='selenium')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "links = soup.select('table.statTable>tbody>tr>td:nth-of-type(2)>a')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "links = soup.select('table.statTable>tbody>tr>td:first-of-type>a')\n",
    "zipcode_tx = [link.text.split()[-1] for link in links]\n",
    "links = soup.select('table.statTable>tbody>tr>td:nth-of-type(2)>a')\n",
    "city_tx = [link.text.split()[-1] for link in links]\n",
    "links = soup.select('table.statTable>tbody>tr>td:nth-of-type(3)>a')\n",
    "county_tx = [link.text.split()[-1] for link in links]\n",
    "links = soup.select('table.statTable>tbody>tr>td:nth-of-type(4)')\n",
    "ziptype = [link.text.split()[-1] for link in links]\n",
    "ziptype =  ziptype[1:]\n",
    "dic_data = {'zipcode': zipcode_tx, 'city': city_tx, 'county': county_tx, 'ziptype': ziptype}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "df_zip = pd.DataFrame(dic_data, columns=['zipcode', 'city', 'county', 'ziptype'])\n",
    "df_zip.to_csv('./data/zipcode_tx.csv', index=False)\n",
    "df_zip.ziptype.unique()\n",
    "df_zip = df_zip[df_zip.ziptype=='Standard']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Redfin data by zip code"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "import sys\n",
    "sys.path.append('./')\n",
    "hou_counties = ['Harris', 'Bend', 'Montgomery', 'Walker']\n",
    "df_zip_hou = df_zip[df_zip.county.isin(hou_counties)]\n",
    "df_zip_hou.shape\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(175, 4)"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "print (df_zip_hou)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "     zipcode       city  county   ziptype\n",
      "1026   77002    Houston  Harris  Standard\n",
      "1027   77003    Houston  Harris  Standard\n",
      "1028   77004    Houston  Harris  Standard\n",
      "1029   77005    Houston  Harris  Standard\n",
      "1030   77006    Houston  Harris  Standard\n",
      "...      ...        ...     ...       ...\n",
      "1403   77562  Highlands  Harris  Standard\n",
      "1409   77571      Porte  Harris  Standard\n",
      "1422   77586   Seabrook  Harris  Standard\n",
      "1423   77587    Houston  Harris  Standard\n",
      "1429   77598    Webster  Harris  Standard\n",
      "\n",
      "[175 rows x 4 columns]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "\n",
    "import mywebdriver\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from random import random\n",
    "import traceback\n",
    "import time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# TEST\n",
    "zipcode = '77007'\n",
    "url_redfin = f'https://www.redfin.com/zipcode/{zipcode}/housing-market'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--window-size=2400,1080\")\n",
    "chrome_options.add_argument(\"--silent\") # open without popup\n",
    "driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "driver.get(url_redfin)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/var/folders/ds/f3qghlvs6z51c03c15bdzhs80000gn/T/ipykernel_1341/340231036.py:4: DeprecationWarning: use options instead of chrome_options\n",
      "  driver = webdriver.Chrome(chrome_options=chrome_options)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "driver.find_element_by_css_selector('div[data-react-server-root-id=\"19\"] .ModeToggler.graphTabs button:first-of-type').click()\n",
    "svg = driver.find_element_by_css_selector('div[data-react-server-root-id=\"19\"] .lineGraph .VictoryContainer>svg')\n",
    "svg = svg.get_attribute('outerHTML')\n",
    "svg = svg.replace('svg width', 'svg xmlns=\"http://www.w3.org/2000/svg\" width')\n",
    "with open(f'image/{zipcode}_1y.svg', 'w') as f:\n",
    "    f.write(svg)\n",
    "driver.find_element_by_css_selector('div[data-react-server-root-id=\"19\"] .ModeToggler.graphTabs button:nth-of-type(3)').click()\n",
    "svg3 = driver.find_element_by_css_selector('div[data-react-server-root-id=\"19\"] .lineGraph .VictoryContainer>svg')\n",
    "svg3 = svg3.get_attribute('outerHTML')\n",
    "svg3 = svg3.replace('svg width', 'svg xmlns=\"http://www.w3.org/2000/svg\" width')\n",
    "with open(f'image/{zipcode}_3y.svg', 'w') as f:\n",
    "    f.write(svg3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "ls_zipdata = []\n",
    "ls_err = []\n",
    "sec_sleep = 20\n",
    "for i, row in df_zip_hou.iterrows():\n",
    "    # initial zip code \n",
    "    zipcode = row['zipcode']\n",
    "    url_redfin = f'https://www.redfin.com/zipcode/{zipcode}/housing-market'\n",
    "    try:\n",
    "        # load web\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        chrome_options.add_argument(\"--silent\") # open without popup\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(url_redfin)\n",
    "        # get price trend svg\n",
    "        driver.find_element_by_css_selector('div[data-react-server-root-id=\"19\"] .ModeToggler.graphTabs button:first-of-type').click()\n",
    "        svg = driver.find_element_by_css_selector('div[data-react-server-root-id=\"19\"] .lineGraph .VictoryContainer>svg')\n",
    "        svg = svg.get_attribute('outerHTML')\n",
    "        svg = svg.replace('svg width', 'svg xmlns=\"http://www.w3.org/2000/svg\" width')\n",
    "        with open(f'image/{zipcode}_1y.svg', 'w') as f:\n",
    "            f.write(svg)\n",
    "        driver.find_element_by_css_selector('div[data-react-server-root-id=\"19\"] .ModeToggler.graphTabs button:nth-of-type(3)').click()\n",
    "        svg3 = driver.find_element_by_css_selector('div[data-react-server-root-id=\"19\"] .lineGraph .VictoryContainer>svg')\n",
    "        svg3 = svg3.get_attribute('outerHTML')\n",
    "        svg3 = svg3.replace('svg width', 'svg xmlns=\"http://www.w3.org/2000/svg\" width')\n",
    "        with open(f'image/{zipcode}_3y.svg', 'w') as f:\n",
    "            f.write(svg3)\n",
    "\n",
    "        # load market insight # soup_zip = mywebdriver.get_soup(url_redfin, driver_type='selenium')\n",
    "        soup_zip = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        driver.quit()\n",
    "        # median price, # of home sold, median DOM, score, \n",
    "        soup_price = soup_zip.select('.MarketInsightsGraphSection:first-of-type button:first-of-type')[0]\n",
    "        soup_sold = soup_zip.select('.MarketInsightsGraphSection:first-of-type button:nth-of-type(2)')[0]\n",
    "        soup_DOM = soup_zip.select('.MarketInsightsGraphSection:first-of-type button:nth-of-type(3)')[0]\n",
    "        median_price = float(soup_price.select('.dataPoints>.value')[0].text.replace('$','').replace(',',''))\n",
    "        median_price_yoy = float(soup_price.select('.dataPoints>.yoyChange')[0].text.replace('+','').replace('%','').replace(' year-over-year', ''))\n",
    "        n_sold = float(soup_sold.select('.dataPoints>.value')[0].text.replace('$','').replace(',',''))\n",
    "        n_sold_yoy = float(soup_sold.select('.dataPoints>.yoyChange')[0].text.replace('+','').replace('%','').replace(' year-over-year', ''))\n",
    "        DOM = float(soup_DOM.select('.dataPoints>.value')[0].text.replace('$','').replace(',',''))\n",
    "        DOM_yoy = float(soup_DOM.select('.dataPoints>.yoyChange')[0].text.replace('+','').replace('%','').replace(' year-over-year', ''))\n",
    "        score = int(soup_zip.select('.scoreTM>.score')[0].text)\n",
    "        dic_zipdata = {'zipcode': zipcode, 'url': url_redfin, 'median_price': median_price, 'median_price_yoy': median_price_yoy, 'n_sold': n_sold, 'n_sold_yoy': n_sold_yoy, 'DOM': DOM, 'DOM_yoy': DOM_yoy, 'redfin_score': score}\n",
    "        ls_zipdata.append(dic_zipdata)\n",
    "    except:\n",
    "        ls_err.append({'zipcode': zipcode, 'url': url_redfin, 'err': traceback.format_exc()})\n",
    "        if len(ls_err) % 5 == 0:\n",
    "            print(f'---- There are {len(ls_err)} errors ----')\n",
    "    time.sleep(random() * sec_sleep)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ds/f3qghlvs6z51c03c15bdzhs80000gn/T/ipykernel_1341/3525994597.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mls_err\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'---- There are {len(ls_err)} errors ----'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msec_sleep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# analyze error\n",
    "df_err = pd.DataFrame(ls_err)\n",
    "df_err = df_err[['zipcode', 'url', 'err']]\n",
    "df_err.to_csv('./data/zipcode_data_err.csv', index=False)\n",
    "df_err.head()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "\"None of [Index(['zipcode', 'url', 'err'], dtype='object')] are in the [columns]\"",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ds/f3qghlvs6z51c03c15bdzhs80000gn/T/ipykernel_1341/4079204267.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# analyze error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mls_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_err\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'zipcode'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'url'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'err'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/zipcode_data_err.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3028\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3029\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3030\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3032\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1264\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1306\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m                 \u001b[0maxis_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m             \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['zipcode', 'url', 'err'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "source": [
    "# check error\n",
    "len(ls_err)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "metadata": {},
     "execution_count": 185
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "source": [
    "# form dataframe\n",
    "df_zipdata = pd.DataFrame(ls_zipdata)\n",
    "df_zipdata = df_zipdata[['zipcode', 'url', 'median_price', 'median_price_yoy', 'n_sold', 'n_sold_yoy', 'DOM', 'DOM_yoy', 'redfin_score']]\n",
    "df_zipdata['median_price_yoy'] = df_zipdata['median_price_yoy']/100\n",
    "df_zipdata['n_sold_yoy'] = df_zipdata['n_sold_yoy']/100\n",
    "df_zipdata['DOM_yoy'] = df_zipdata['DOM_yoy']/100\n",
    "df_zipdata.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(154, 9)"
      ]
     },
     "metadata": {},
     "execution_count": 190
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "source": [
    "# get image path\n",
    "def get_image_path(x):\n",
    "    return f'./image/{x}_1y.svg'\n",
    "def get_image_path3(x):\n",
    "    return f'./image/{x}_3y.svg'\n",
    "df_zipdata['svg1'] = df_zipdata.zipcode.map(get_image_path)\n",
    "df_zipdata['svg3'] = df_zipdata.zipcode.map(get_image_path3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "source": [
    "# save csv\n",
    "df_zipdata.to_csv('./data/zipcode_data.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  },
  "orig_nbformat": 3,
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}